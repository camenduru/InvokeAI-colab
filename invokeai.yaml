InvokeAI:
  Web Server:
    host: 127.0.0.1
    port: 9090
    allow_origins: []
    allow_credentials: true
    allow_methods:
    - '*'
    allow_headers:
    - '*'
  Features:
    esrgan: true
    internet_available: true
    log_tokenization: false
    patchmatch: true
    ignore_missing_core_models: false
  Paths:
    autoimport_dir: autoimport
    lora_dir: null
    embedding_dir: null
    controlnet_dir: null
    conf_path: configs/models.yaml
    models_dir: models
    legacy_conf_dir: configs/stable-diffusion
    db_dir: databases
    outdir: outputs
    use_memory_db: false
    custom_nodes_dir: nodes
  Logging:
    log_handlers:
    - console
    log_format: color
    log_level: info
    log_sql: false
  Development:
    dev_reload: false
  Model Cache:
    ram: 5.5
    vram: 14
    lazy_offload: false
    log_memory_usage: false
  Device:
    device: auto
    precision: auto
  Generation:
    sequential_guidance: false
    attention_type: auto
    attention_slice_size: auto
    force_tiled_decode: false
    png_compress_level: 6
  Queue:
    max_queue_size: 10000
  Nodes:
    allow_nodes: null
    deny_nodes: null
    node_cache_size: 512
